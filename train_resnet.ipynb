{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "from resnet import *\n",
    "from baseline_cnn import *\n",
    "from resnet import resnet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pathlib\n",
    "import torch\n",
    "from evaluation import Evaluation\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 16          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "class channelCopy(object):    \n",
    "    def __call__(self, img):\n",
    "        return torch.cat([img, img, img], 0)\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "transform = transforms.Compose([transforms.Resize(512),transforms.ToTensor(),channelCopy()])\n",
    "\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "# train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "#                                                              p_val=p_val, p_test=p_test,\n",
    "#                                                              shuffle=True, show_sample=False, \n",
    "#                                                              extras=extras)\n",
    "train_loader, val_loader, test_loader = create_balanced_split_loaders(batch_size, seed, transform=transform,\n",
    "                                                                         p_val=p_val, p_test=p_test,\n",
    "                                                                         shuffle=True, show_sample=False,\n",
    "                                                                         extras=extras, z_score=conf['z_score'])\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = resnet50(pretrained=False, num_classes=14)\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "#criterion = nn.MultiLabelSoftMarginLoss() #TODO - loss criteria are defined in the torch.nn package\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.00001) #TODO - optimizers are defined in the torch.optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder for model saving\n",
    "model_path = '{}/models/resnet50/{}/'.format(os.getcwd(), time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "model_pathlib = pathlib.Path(model_path)\n",
    "if not model_pathlib.exists():\n",
    "    pathlib.Path(model_pathlib).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#save loss in files for futher usage.\n",
    "loss_path = '{}/losses/resnet50/{}/'.format(os.getcwd(), time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "loss_pathlib = pathlib.Path(loss_path)\n",
    "if not loss_pathlib.exists():\n",
    "    pathlib.Path(loss_pathlib).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 tensor(0.6917, device='cuda:0')\n",
      "Epoch 1, average minibatch 0 loss: 0.013\n",
      "val 50 tensor(0.1851, device='cuda:0')\n",
      "Epoch 1, average minibatch 50 loss: 0.391\n",
      "val 100 tensor(0.1819, device='cuda:0')\n",
      "Epoch 1, average minibatch 100 loss: 0.189\n",
      "val 150 tensor(0.1817, device='cuda:0')\n",
      "Epoch 1, average minibatch 150 loss: 0.178\n",
      "val 200 tensor(0.1833, device='cuda:0')\n",
      "Epoch 1, average minibatch 200 loss: 0.183\n",
      "val 250 tensor(0.1820, device='cuda:0')\n",
      "Epoch 1, average minibatch 250 loss: 0.180\n",
      "val 300 tensor(0.1821, device='cuda:0')\n",
      "Epoch 1, average minibatch 300 loss: 0.179\n",
      "val 350 tensor(0.1813, device='cuda:0')\n",
      "Epoch 1, average minibatch 350 loss: 0.185\n",
      "val 400 tensor(0.1846, device='cuda:0')\n",
      "Epoch 1, average minibatch 400 loss: 0.187\n",
      "val 450 tensor(0.1808, device='cuda:0')\n",
      "Epoch 1, average minibatch 450 loss: 0.192\n",
      "val 500 tensor(0.1810, device='cuda:0')\n",
      "Epoch 1, average minibatch 500 loss: 0.196\n",
      "val 550 tensor(0.1805, device='cuda:0')\n",
      "Epoch 1, average minibatch 550 loss: 0.175\n",
      "val 600 tensor(0.1821, device='cuda:0')\n",
      "Epoch 1, average minibatch 600 loss: 0.185\n",
      "val 650 tensor(0.1811, device='cuda:0')\n",
      "Epoch 1, average minibatch 650 loss: 0.189\n",
      "val 700 tensor(0.1809, device='cuda:0')\n",
      "Epoch 1, average minibatch 700 loss: 0.181\n",
      "val 750 tensor(0.1815, device='cuda:0')\n",
      "Epoch 1, average minibatch 750 loss: 0.176\n",
      "val 800 tensor(0.1807, device='cuda:0')\n",
      "Epoch 1, average minibatch 800 loss: 0.180\n",
      "val 850 tensor(0.1813, device='cuda:0')\n",
      "Epoch 1, average minibatch 850 loss: 0.183\n",
      "val 900 tensor(0.1808, device='cuda:0')\n",
      "Epoch 1, average minibatch 900 loss: 0.183\n",
      "val 950 tensor(0.1809, device='cuda:0')\n",
      "Epoch 1, average minibatch 950 loss: 0.174\n",
      "val 1000 tensor(0.1808, device='cuda:0')\n",
      "Epoch 1, average minibatch 1000 loss: 0.198\n",
      "val 1050 tensor(0.1800, device='cuda:0')\n",
      "Epoch 1, average minibatch 1050 loss: 0.182\n",
      "val 1100 tensor(0.1807, device='cuda:0')\n",
      "Epoch 1, average minibatch 1100 loss: 0.183\n",
      "val 1150 tensor(0.1794, device='cuda:0')\n",
      "Epoch 1, average minibatch 1150 loss: 0.173\n"
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "loss_val_list = []\n",
    "loss_val_min = float('inf')\n",
    "N = 50\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    N_minibatch_loss = 0.0    \n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "#         if minibatch_count == 100:\n",
    "#             break\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print('training',minibatch_count,loss)\n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "        #TODO: Implement validation\n",
    "        if minibatch_count % N == 0:\n",
    "            #switch to evaluate mode\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                loss_val = 0\n",
    "                for count_val, (images_val, labels_val) in enumerate(val_loader):\n",
    "#                     if count_val ==10:\n",
    "#                         break\n",
    "                    images_val, labels_val = images_val.to(computing_device), labels_val.to(computing_device)\n",
    "                    outputs_val = model(images_val)\n",
    "                    loss_val += criterion(outputs_val, labels_val)\n",
    "#                     print('val',count_val, (loss_val/(count_val+1)))\n",
    "                loss_val /= count_val\n",
    "                print('val',minibatch_count,loss_val)\n",
    "                loss_val_list.append(loss_val.item())\n",
    "                if loss_val < loss_val_min:\n",
    "                    model_name = \"epoch_{}-batch_{}-loss_{}-{}.pt\".format(epoch, minibatch_count, loss_val, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "                    torch.save(model.state_dict(), os.path.join(model_path, model_name))\n",
    "                    loss_val_min = loss_val\n",
    "                    \n",
    "        if minibatch_count % N == 0:    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss.item())\n",
    "            N_minibatch_loss = 0.0\n",
    "            \n",
    "        prefix = 'resnet50_loss_'\n",
    "        with open(os.path.join(loss_path, prefix+\"training.txt\"), \"w\") as f:\n",
    "            for s in total_loss:\n",
    "                f.write(str(s) +\"\\n\")\n",
    "\n",
    "        with open(os.path.join(loss_path, prefix+\"training_ave.txt\"), \"w\") as f:\n",
    "            for s in avg_minibatch_loss:\n",
    "                f.write(str(s) +\"\\n\")\n",
    "\n",
    "        with open(os.path.join(loss_path, prefix+\"val.txt\"), \"w\") as f:\n",
    "            for s in loss_val_list:\n",
    "                f.write(str(s) +\"\\n\")\n",
    "            \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch, \"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/datasets/home/home-02/60/960/kshi/PA3/models/resnet50/20190214-113501/epoch_0-batch_1900-loss_0.17764276266098022-20190214-154931.pt'\n",
    "model_test = resnet50(pretrained=False, num_classes=14)\n",
    "model_test = model_test.to(computing_device)\n",
    "model_test.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = []\n",
    "predictions_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        labels_all.append(labels)\n",
    "        output = model_test(images)\n",
    "        predictions = output > 0.5\n",
    "        predictions_all.append(predictions)\n",
    "\n",
    "labels = torch.cat(labels_all,0)\n",
    "predctions = torch.cat(predictions_all,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP [2088.0, 0.0, 2400.0, 0.0, 1045.0, 1170.0, 0.0, 990.0, 0.0, 0.0, 445.0, 305.0, 143.0, 0.0]\n",
      "FP [18093.0, 0.0, 17781.0, 0.0, 19136.0, 19011.0, 0.0, 19191.0, 0.0, 0.0, 19736.0, 19876.0, 4828.0, 0.0]\n",
      "TN [0.0, 19642.0, 0.0, 16574.0, 0.0, 0.0, 19954.0, 0.0, 19340.0, 19750.0, 0.0, 0.0, 14763.0, 20151.0]\n",
      "FN [0.0, 539.0, 0.0, 3607.0, 0.0, 0.0, 227.0, 0.0, 841.0, 431.0, 0.0, 0.0, 447.0, 30.0]\n",
      "tensor([0.1035, 0.9733, 0.1189, 0.8213, 0.0518, 0.0580, 0.9888, 0.0491, 0.9583,\n",
      "        0.9786, 0.0221, 0.0151, 0.7386, 0.9985], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor(0.4911, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluation(predctions.float(), labels)\n",
    "print(eval.accuracy())\n",
    "print(eval.accuracy().mean())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1035,    nan, 0.1189,    nan, 0.0518, 0.0580,    nan, 0.0491,    nan,\n",
       "           nan, 0.0221, 0.0151, 0.0288,    nan], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        0.0000, 1.0000, 1.0000, 0.2424, 0.0000], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.recall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
